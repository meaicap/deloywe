from rag.vector_store import load_vector_db
from core.llm import client

# =====================
# CONFIG
# =====================
MAX_CONTEXT_CHARS = 4000
MIN_CONTEXT_CHARS = 200
MIN_CHUNK_LENGTH = 40


# ======================================================
# CORE: RETRIEVE CONTEXT BY DOCUMENT_ID (üî• QUAN TR·ªåNG)
# ======================================================
def retrieve_context_by_document(
    document_id: int,
    query: str,
    k: int = 6
) -> str:
    """
    L·∫•y ng·ªØ c·∫£nh h·ªçc t·∫≠p t·ª´ VectorDB (RAG)
    ‚Üí CH·ªà L·∫§Y CHUNK C·ª¶A document_id ƒê∆Ø·ª¢C CH·ªåN
    """

    try:
        vectordb = load_vector_db()

        docs = vectordb.similarity_search(
            query,
            k=k,
            filter={
                "document_id": document_id
            }
        )

    except Exception as e:
        print("‚ùå RAG search error:", e)
        return ""

    if not docs:
        print(f"‚ö†Ô∏è RAG: Kh√¥ng t√¨m th·∫•y chunk cho document_id={document_id}")
        return ""

    contexts = []
    total_chars = 0

    for doc in docs:
        content = doc.page_content.strip()

        if not content:
            continue

        if len(content) < MIN_CHUNK_LENGTH:
            continue

        contexts.append(content)
        total_chars += len(content)

        if total_chars >= MAX_CONTEXT_CHARS:
            break

    final_context = "\n\n".join(contexts)

    # DEBUG
    print("====== RAG CONTEXT PREVIEW ======")
    print(final_context[:800])
    print("====== END CONTEXT ======")

    if len(final_context) < MIN_CONTEXT_CHARS:
        print("‚ö†Ô∏è Context qu√° ng·∫Øn:", len(final_context))
        return ""

    return final_context


# ======================================================
# BACKWARD COMPAT (N·∫æU SAU N√ÄY C·∫¶N RAG GLOBAL)
# ======================================================
def retrieve_context(query: str, k: int = 6) -> str:
    """
    RAG kh√¥ng g·∫Øn document_id (d·ª± ph√≤ng)
    """
    try:
        vectordb = load_vector_db()
        docs = vectordb.similarity_search(query, k=k)
    except Exception as e:
        print("‚ùå RAG search error:", e)
        return ""

    contexts = []
    total_chars = 0

    for doc in docs:
        content = doc.page_content.strip()
        if not content or len(content) < MIN_CHUNK_LENGTH:
            continue

        contexts.append(content)
        total_chars += len(content)
        if total_chars >= MAX_CONTEXT_CHARS:
            break

    final_context = "\n\n".join(contexts)
    if len(final_context) < MIN_CONTEXT_CHARS:
        return ""

    return final_context


# ======================================================
# RAG ANSWER (D√ôNG CHO CHATBOX)
# ======================================================
def rag_answer(
    question: str,
    document_id: int
) -> str:
    """
    Tr·∫£ l·ªùi d·ª±a tr√™n RAG
    ‚Üí G·∫ÆN CH·∫∂T THEO document_id
    """

    retrieve_query = f"""
    Kh√°i ni·ªám, ƒë·ªãnh nghƒ©a, nguy√™n l√Ω, c√¥ng th·ª©c,
    n·ªôi dung h·ªçc t·∫≠p quan tr·ªçng li√™n quan ƒë·∫øn:
    {question}
    """

    context = retrieve_context_by_document(
        document_id=document_id,
        query=retrieve_query
    )

    if not context:
        return "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu."

    prompt = f"""
B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p.

NHI·ªÜM V·ª§:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin trong t√†i li·ªáu
- Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, R√ï R√ÄNG
- D·∫°ng h·ªçc t·∫≠p (ƒë·ªãnh nghƒ©a, gi·∫£i th√≠ch, g·∫°ch ƒë·∫ßu d√≤ng)
- KH√îNG d√πng c√¢u chung chung
- KH√îNG t·ª± suy ƒëo√°n

T√ÄI LI·ªÜU:
\"\"\" 
{context}
\"\"\" 

C√ÇU H·ªéI:
{question}

TR·∫¢ L·ªúI:
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1
    )

    return response.choices[0].message.content.strip()
